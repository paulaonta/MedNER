{"cells":[{"cell_type":"markdown","metadata":{"id":"0kiPhSy9iX6s"},"source":["# MedNER\n","---  \n","  \n","Medikuntzako NER sistema gainbegiratu bat egingo da.\n","\n","Horretarako, lehenik eta behin beharrezko liburutegiak inportatuko dira. Erabiliko da the Hugging Face Transformers liburutegia zeinak diseinatuta dagoen aurre-entrenatutako Transformer modeloak, BERT adibidez, erabiltzeko. Instalatzeko:\n"]},{"cell_type":"code","execution_count":39,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4579,"status":"ok","timestamp":1670358947657,"user":{"displayName":"Paula Ontalvilla","userId":"15665637065710464846"},"user_tz":-60},"id":"Mm5TUujDedY_","outputId":"3a6f4c4e-ccf0-49e5-a516-8a73d5bbc27b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.25.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.11.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.9.24)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n"]}],"source":["!pip install transformers"]},{"cell_type":"markdown","metadata":{"id":"6zXg4dbSjaD2"},"source":["Gainerako liburutegiak inportatuko dira:"]},{"cell_type":"code","execution_count":40,"metadata":{"executionInfo":{"elapsed":44,"status":"ok","timestamp":1670358947661,"user":{"displayName":"Paula Ontalvilla","userId":"15665637065710464846"},"user_tz":-60},"id":"HQRlV1R1jZco"},"outputs":[],"source":["from transformers import BertModel, BertTokenizer, BertTokenizerFast\n","import torch\n","from tqdm import tqdm\n","import torch.nn as nn\n","import numpy as np\n","import random\n","import time\n","import spacy\n","import os"]},{"cell_type":"markdown","metadata":{"id":"a-m9mX-4jhe6"},"source":["Konprobatuko da notebook-a GPUan exekutatzen dagoela (\"Running on cuda\" inprimatu beharko luke)."]},{"cell_type":"code","execution_count":41,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":42,"status":"ok","timestamp":1670358947664,"user":{"displayName":"Paula Ontalvilla","userId":"15665637065710464846"},"user_tz":-60},"id":"QyKZu3smkaRB","outputId":"1d38ce3f-eb43-46ba-b251-246b07ed38d3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Running on cpu\n"]}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Running on {}\".format(device))"]},{"cell_type":"markdown","metadata":{"id":"AlBYV_YydmAn"},"source":["Datuak dituen direktoria erabiliko da:"]},{"cell_type":"code","execution_count":42,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2922,"status":"ok","timestamp":1670358950551,"user":{"displayName":"Paula Ontalvilla","userId":"15665637065710464846"},"user_tz":-60},"id":"p4nx0eL5cuTd","outputId":"cf26f402-7075-4e62-b651-6e41dbb6db81"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/MyDrive/universidad/4.CURSO/HP/PROIEKTUA/data\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","%cd \"/content/drive/MyDrive/universidad/4.CURSO/HP/PROIEKTUA/data/\""]},{"cell_type":"markdown","metadata":{"id":"Ggavyi8rs1Qv"},"source":["## Datuak irakurri eta *train*, *dev* eta *test* partizioak egin\n","\n","Anotatutako corpus guztia *corpus_pubtator.txt* fitxategian dago.  Datu hauek PubTator formatuan daude. Zehazki kasu bakoitza honela dago errepresentatuta:\n","  \n","```\n","PMID | t | Title text  \n","PMID | a | Abstract text    \n","PMID TAB StartIndex TAB EndIndex TAB MentionTextSegment TAB SemanticTypeID TAB EntityID\n","...\n","```\n","\n","Lehenengo bi lerroek izenburuko eta laburpeneko testuak aurkezten dituzte (lerro-jauzirik eta tabulaziorik gabe testuan). Hurrengo lerroek aipamenak aurkezten dituzte, lerro bakoitzeko bana. *StartIndex* eta *EndIndex* dokumentuaren testuan 0an oinarritutako karaktereen aurkibideak dira, Izenburua eta Laburpena kateatuz eraikiak, SPACE karaktere batez bereiziak. *MentionTextSegment* karaktere-posizio horien arteko benetako aipamena da. *EntityID* UMLS entitatearen id da (kontzeptua), eta *SemanticTypeID* entitatea UMLSen lotuta dagoen mota semantikoaren id da. UMLS entitate mota semantiko bati baino gehiagori lotuta badago, eremu honek komen arabera bereizitako zerrenda bat du ID mota guztiekin. 2017-AA bertsio aktiboan ez dauden UMLS kontzeptu guztiak *UnknownType* mota semantiko bereziari lotuta daude.  \n","\n","Jarraian adibide bat azaltzen da:\n","```\n","25763772|t|DCTN4 as a modifier of chronic Pseudomonas aeruginosa infection in cystic fibrosis\n","25763772|a|Pseudomonas aeruginosa (Pa) infection in cystic fibrosis (CF) patients is associated with worse long-term pulmonary disease and shorter survival, and chronic Pa infection (CPA) is associated with reduced lung function, faster rate of lung decline, increased rates of exacerbations and shorter survival. By using exome sequencing and extreme phenotype design, it was recently shown that isoforms of dynactin 4 (DCTN4) may influence Pa infection in CF, leading to worse respiratory disease. The purpose of this study was to investigate the role of DCTN4 missense variants on Pa infection incidence, age at first Pa infection and chronic Pa infection incidence in a cohort of adult CF patients from a single centre. Polymerase chain reaction and direct sequencing were used to screen DNA samples for DCTN4 variants. A total of 121 adult CF patients from the Cochin Hospital CF centre have been included, all of them carrying two CFTR defects: 103 developed at least 1 pulmonary infection with Pa, and 68 patients of them had CPA. DCTN4 variants were identified in 24% (29/121) CF patients with Pa infection and in only 17% (3/18) CF patients with no Pa infection. Of the patients with CPA, 29% (20/68) had DCTN4 missense variants vs 23% (8/35) in patients without CPA. Interestingly, p.Tyr263Cys tend to be more frequently observed in CF patients with CPA than in patients without CPA (4/68 vs 0/35), and DCTN4 missense variants tend to be more frequent in male CF patients with CPA bearing two class II mutations than in male CF patients without CPA bearing two class II mutations (P = 0.06). Our observations reinforce that DCTN4 missense variants, especially p.Tyr263Cys, may be involved in the pathogenesis of CPA in male CF.\n","25763772        0       5       DCTN4   T116,T123    C4308010\n","25763772        23      63      chronic Pseudomonas aeruginosa infection        T047    C0854135\n","25763772        67      82      cystic fibrosis T047    C0010674\n","25763772        83      120     Pseudomonas aeruginosa (Pa) infection   T047    C0854135\n","...\n","```\n","\n","Bestalde, *train*, *dev* eta *test* partizioak *corpus_pubtator_pmids_trng.txt, corpus_pubtator_pmids_dev.txt, corpus_pubtator_pmids_test.txt* fitxategietan daude.  Hauek dokumentuaren  %60, %20, %20-ko ausazko partizioak dituzte, hurrenez hurren. Bakoitzean agertzen dira PMID kodeak zehaztuz bakoitzean dauden kasue kodeak.  \n","\n","Beraz, partizioak egingo dira dokumentu bana izateko *train*, *dev* eta *test* kasuetarako. Horretarako:\n","1. Sortuko da fitxategi bana *train*, *dev* eta *test*-eko kasuak gordetzeko.\n","2. Irakurriko dira *corpus_pubtator_pmids_trng.txt, corpus_pubtator_pmids_dev.txt, corpus_pubtator_pmids_test.txt* fitxategiak. Irakurtzen den kode bakoitzeko *corpus_pubtator.txt* fitxategian  dagoen kasu hori hartuko da eta dagokion fitxategian (*train*, *dev* edo *test*) kopiatuko da."]},{"cell_type":"markdown","metadata":{"id":"cMZGRDq3z4oZ"},"source":["### Aldagai orokorrak definitu direktorioak gordetzeko"]},{"cell_type":"code","execution_count":43,"metadata":{"executionInfo":{"elapsed":25,"status":"ok","timestamp":1670358950553,"user":{"displayName":"Paula Ontalvilla","userId":"15665637065710464846"},"user_tz":-60},"id":"XXqtfs4as8J5"},"outputs":[],"source":["data_directory = \"corpus_pubtator.txt\"\n","\n","train_directory = \"corpus_pubtator_pmids_trng.txt\"\n","dev_directory = \"corpus_pubtator_pmids_dev.txt\"\n","test_directory = \"corpus_pubtator_pmids_test.txt\"\n","\n","train_def_directory = \"corpus_pubtator_train.txt\"\n","dev_def_directory = \"corpus_pubtator_dev.txt\"\n","test_def_directory = \"corpus_pubtator_test.txt\""]},{"cell_type":"markdown","metadata":{"id":"D0-cQue30BA6"},"source":["### Fitxategiak sortu"]},{"cell_type":"code","execution_count":44,"metadata":{"id":"wanbVz6M0WTM","executionInfo":{"status":"ok","timestamp":1670358950554,"user_tz":-60,"elapsed":21,"user":{"displayName":"Paula Ontalvilla","userId":"15665637065710464846"}}},"outputs":[],"source":["def createFile(path):\n","    if not os.path.exists(path):\n","        os.makedirs(os.path.dirname(path), exist_ok=True)\n","\n","def createDirectory(path):\n","    if not os.path.exists(path):\n","        os.mkdir(path)\n","\n","createFile(\"./\" + train_def_directory)\n","createFile(\"./\" + dev_def_directory)\n","createFile(\"./\" + test_def_directory)"]},{"cell_type":"markdown","metadata":{"id":"dob-JaqL-_Bd"},"source":["### Fitxategiak idatzi"]},{"cell_type":"code","execution_count":45,"metadata":{"executionInfo":{"elapsed":20,"status":"ok","timestamp":1670358950556,"user":{"displayName":"Paula Ontalvilla","userId":"15665637065710464846"},"user_tz":-60},"id":"mvGe0Mg3_E86"},"outputs":[],"source":["def idatzi_kasua(write_directory, line):\n","  with open(write_directory, \"a\") as f:\n","    f.write(line)"]},{"cell_type":"markdown","metadata":{"id":"FbqcErWc0S8_"},"source":["### Fitxategiak irakurri"]},{"cell_type":"code","execution_count":46,"metadata":{"id":"zJNDk6cc6ntf","executionInfo":{"status":"ok","timestamp":1670353044909,"user_tz":-60,"elapsed":10,"user":{"displayName":"Paula Ontalvilla","userId":"15665637065710464846"}}},"outputs":[],"source":["def aurkitu_kasua(data_directory,write_directory, code):\n","  aurkitu = False\n","  with open(data_directory) as file:\n","        for line in file:\n","          if  \"|\" in line:\n","            lines = line.split(\"|\")\n","          else:\n","            lines = line.split(\"\\t\")\n","            \n","          if lines[0] == code:\n","            aurkitu = True\n","            idatzi_kasua(write_directory, line)\n","          elif aurkitu:\n","            break"]},{"cell_type":"code","execution_count":47,"metadata":{"id":"4PIpTBol86Ka","executionInfo":{"status":"ok","timestamp":1670353046050,"user_tz":-60,"elapsed":8,"user":{"displayName":"Paula Ontalvilla","userId":"15665637065710464846"}}},"outputs":[],"source":["def read_makePartitions_txt(filename, write_directory):\n","    \n","    \"\"\" Read input one by line \"\"\"\n","    with open(filename) as file:\n","        for line in file:\n","            aurkitu_kasua(data_directory, write_directory, line.strip())"]},{"cell_type":"code","source":["read_makePartitions_txt(train_directory, train_def_directory)\n","read_makePartitions_txt(dev_directory, dev_def_directory)\n","read_makePartitions_txt(test_directory, test_def_directory)"],"metadata":{"id":"Ad6DKSUXugYC","executionInfo":{"status":"aborted","timestamp":1670353019505,"user_tz":-60,"elapsed":48,"user":{"displayName":"Paula Ontalvilla","userId":"15665637065710464846"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1Brx5hHBoe7s"},"source":["## Z1\n","MedMentions corpusa erabiliz NER sistema orokor bat\n","entrenatu, termino bat UMLSkoa den, hau da, osasunarekin zerikusia ote duen\n","jakiteko. Hau da, terminoak identifikatu, klase bakarra irteeran duzularik\n","(Medikuntzkoa edo MED).  \n","\n","[DCTN4] as a modifier of [chronic Pseudomonas aeruginosa infection] in\n","[cystic fibrosis]"]},{"cell_type":"markdown","metadata":{"id":"0KzhfYxK8iBh"},"source":["### BIO etiketatzea\n","Z1 betetzeko, partizioak eginda daudela hauetan dauden kasuak MedMentions-eko formatutik BIO etiketatzera egokituko dira. Aurrerago erabiliko diren ereduek formatu hau eskatzen dutelako.     \n","\n","\n","Etiketa bakarra egongo da, MED dena. Beraz, tokena entitate baten hasiera baldin bada B-MED etiketa jasoko du. I-BER izango du aldiz, entitatearen barruan baldin badago eta 0 ez bada entitate bat. Jarraian adibide bat aurkezten da:\n","\n","```\n","[DCTN4] as a modifier of [chronic Pseudomonas aeruginosa infection] in [cystic fibrosis]\n","\n","B-MED O O O O B-MED I-MED I-MED I-MED O B-MED I-MED\n","```\n","\n","Lehenik eta behin fitxategi berriak sortuko dira *train*, *dev* eta *test* partizio bakoitzeko kasuak BIO etiketatzean gordetzeko."]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":24,"status":"ok","timestamp":1670357868236,"user":{"displayName":"Paula Ontalvilla","userId":"15665637065710464846"},"user_tz":-60},"id":"B4HXnih0ka6x"},"outputs":[],"source":["#Aldagai orokorrak\n","train_BIO_directory = \"corpus_BIO_train.txt\"\n","dev_BIO_directory = \"corpus_BIO_dev.txt\"\n","test_BIO_directory = \"corpus_BIO_test.txt\"\n","\n","#sortu fitxategiak\n","createFile(\"./\" + train_BIO_directory)\n","createFile(\"./\" + dev_BIO_directory)\n","createFile(\"./\" + test_BIO_directory)"]},{"cell_type":"markdown","metadata":{"id":"7UylYigo_kld"},"source":["Orain aurretik sortu diren fitxategiak (corpus_pubtator_train.txt, corpus_pubtator_test.txt eta corpus_pubtator_dev.txt) irakurri dira eta etiketatuko dira. Horretarako kasu bakoitzeko lehenengo bi lerroak irakurriko dira izenburua eta deskripzioa direnak, hain zuzen ere. Hauek etiketatuko dira O jarriz baldin eta hurrengo lerroetan ez badaude. Izan ere, lerro bana dago entitate bakoitzeko. B-MED jarriko da baldin eta entitate baten hasiera bada eta I-MED entitate baten parte bada. Horretarako, hurrengo lerroak begiratuko dira eta aztertuko da *MentionTextSegment* eta *StartIndex* jakiteko entitatearen hasiera edo parte den.  \n","\n","Bestalde, *SpaCy*-ko tokenizatzailea erabiliko da titulua eta abstracta tokenizatzeko. Gainera, *MentionTextSegment* hainbat hitzez osatuta egon daitekenez baita erabiliko da hau tokenizatzeko.\n","\n","SpaCy-ko inportak egingo eta tokenizatzailea definituko da:"]},{"cell_type":"code","source":["# spaCy-ko lematizatzailea behar dugu baina gainerakoa kenduko dugu analisi-katetik\n","nlp = spacy.load('en_core_web_sm', disable=['tagger,ner,parser'])\n","nlp.remove_pipe('tagger')\n","nlp.remove_pipe('ner')\n","nlp.remove_pipe('parser');\n","\n","def tokenize(text):\n","  new_text = \"\"\n","  for t in text:\n","    if t == \" \":\n","      new_text += \"  \"\n","    else:\n","      new_text += t\n","\n","  spacy_tokens=[]\n","  spacy_tokens.extend([token.text for token in nlp(new_text)])\n","  return spacy_tokens\n"],"metadata":{"id":"1JI8K1bkHeh4","executionInfo":{"status":"ok","timestamp":1670357870098,"user_tz":-60,"elapsed":1885,"user":{"displayName":"Paula Ontalvilla","userId":"15665637065710464846"}}},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":["Fitxategi bakoitza irakurri eta BIO moduan etiketatu klase bat daukagula bakarrik:"],"metadata":{"id":"7LugtzNq1ayF"}},{"cell_type":"code","source":["def idatzi_entitateak(entitate_guztiak, write_directory):\n","  for entitatea in entitate_guztiak:\n","    if entitatea[0] != \" \":\n","      text = entitatea[0] + \"\\t\" + entitatea[3] + \"\\n\"\n","      idatzi_kasua(write_directory, text)\n"],"metadata":{"id":"it6JD7ZXp9op","executionInfo":{"status":"ok","timestamp":1670357870100,"user_tz":-60,"elapsed":46,"user":{"displayName":"Paula Ontalvilla","userId":"15665637065710464846"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["def readBIO_txt(filename, write_directory):\n","    \n","    \"\"\" Read input one by line \"\"\"\n","    with open(filename) as file:\n","        lines = \"\" #it  will contain the tittle and abstract\n","        entitate_guztiak = []\n","        cont = 0\n","        cont_len = 0\n","        for line in file:\n","          if  \"|\" in line:\n","            if cont % 2 == 0:\n","              #idatzi\n","              idatzi_entitateak(entitate_guztiak, write_directory)\n","              entitate_guztiak = []\n","              lines = \"\"           \n","              cont_len = 0   \n","            cont += 1\n","            text_ta = (line.split(\"|\")[2]).split(\"\\n\")[0]\n","            tokenizatu_text_ta = tokenize(text_ta+ \" \")\n","            for t in tokenizatu_text_ta:\n","              entitate_guztiak.append([t,cont_len , cont_len+len(t) ,\"O\"])\n","              cont_len += len(t)\n","            lines += text_ta + \" \"\n","          else:\n","            desk = line.split(\"\\t\")\n","            index_start = int(desk[1])\n","            index_end = int(desk[2])\n","            entitate = lines[index_start:index_end]\n","            entitate_tokenizatu = tokenize(entitate)\n","            first = True\n","            for ent in entitate_tokenizatu:\n","              #print(str(ent)+\" \"+str(index_start)+ \" \"+str(index_start + len(ent)))\n","              if first:\n","                if ent != \" \":\n","                  try:\n","                    index = entitate_guztiak.index([ent,index_start, index_start + len(ent), \"O\"])\n","                    entitate_guztiak[index] = [ent,index_start, index_start + len(ent), \"B-MED\"]\n","                    index_start += len(ent)\n","                    first = False\n","                  except:\n","                    index += 1\n","                    entitate_guztiak.insert(index, [ent, len(entitate_guztiak[index-1][0]), len(entitate_guztiak[index-1][0])+len(ent), \"B-MED\"])\n","                    index_start += len(ent)\n","                    first = False\n","                else:\n","                  index_start += 1\n","              elif ent != \" \":\n","                try:\n","                  index = entitate_guztiak.index([ent,index_start, index_start +len(ent), \"O\"])\n","                  entitate_guztiak[index] = [ent,index_start, index_start +len(ent), \"I-MED\"]\n","                  index_start += len(ent)\n","                except:\n","                  index += 1\n","                  entitate_guztiak.insert(index, [ent, len(entitate_guztiak[index-1][0]), len(entitate_guztiak[index-1][0])+len(ent), \"I-MED\"])\n","                  index_start += len(ent)\n","              else:\n","                index_start += 1\n"],"metadata":{"id":"ZJBktXUXpQwd","executionInfo":{"status":"ok","timestamp":1670365447946,"user_tz":-60,"elapsed":429,"user":{"displayName":"Paula Ontalvilla","userId":"15665637065710464846"}}},"execution_count":73,"outputs":[]},{"cell_type":"code","source":["readBIO_txt(dev_def_directory ,dev_BIO_directory)\n","readBIO_txt(test_def_directory ,test_BIO_directory)\n","readBIO_txt(train_def_directory ,train_BIO_directory)"],"metadata":{"id":"9a6Q_T1dVgTG","executionInfo":{"status":"ok","timestamp":1670370661482,"user_tz":-60,"elapsed":5211982,"user":{"displayName":"Paula Ontalvilla","userId":"15665637065710464846"}}},"execution_count":74,"outputs":[]},{"cell_type":"markdown","source":["### BIO moduan etiketatutako datuak irakurri\n","\n","Jarrain definituko dira funtzio batzuk aurretik etiketatu diren datuak irakurtzeko. Funtzio honek datuak irakurri eta esaldiz osatutako lista bat sortzen du. Esaldi bakoitza tuplas osatutako lista bat da eta tupla bakoitz bi elementuz osatuta dago: hitza eta etiketa (O,B,I).\n"],"metadata":{"id":"St_IisJWw5EH"}},{"cell_type":"code","source":["def read_txt(filename):    \n","    sentences=[]\n","    sentence=[]\n","    with open(filename) as file:\n","        for line in file:\n","            cols=line.rstrip().split(\"\\t\")\n","            if len(cols) < 2:\n","                if len(sentence) > 0:\n","                    sentences.append(sentence)\n","                sentence=[]\n","                continue\n","                \n","            word=cols[0]\n","            tag=cols[1]\n","            \n","            sentence.append((word, tag))\n","            \n","        if len(sentence) > 0:\n","            sentences.append(sentence)\n","            \n","    return sentences"],"metadata":{"id":"_ntfw3JyyEPf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","###BERT-base sekuentzia-etiketatzailea definitzea\n","\n","Kasu honetan, tokenizatzailea eta transformerraren parametroak deskargatuko dira Transformers liburutegitik. Modeloak \"model_name\" zehatz batekin gordetzen dira.\n","\n","Mini-batchak tokenizatzailearen arabera antolatzen dira.\n","\n","Hugging Face (BertForTokenClassification) modeloa erabili beharrean, BERTek NERrako sekuentzia-sailkatzaile gisa funtzionatzeko kodea sartzen dugu. Arrazoia da tokenizazio azpihitzetik eratorritako konplikazioak agerian uztea.\n","\n","NER datasetek hitz bakoitzari BIO-formatuko etiketa bat ematen dion bitartean, BERT tokenizatzaileak hitz horiek token askotan bana ditzake. Horrela, etiketak egokitu behar ditugu sekuentzia tokenizatuaren luzera berriarekin bat etortzeko. Hau egiteko modu asko daude: jatorrizko hitzaren BIO etiketa dagozkion fitxa guztiei eslei dakieke, edo, bestela, BIO etiketa lehenengo azpihitzari esleitu, eta, aldi berean, etiketa berezi bat (-100) esleitu hitz horri dagozkion gainerako azpihitzei. Lehengo aukera erabiliko da, eta etiketak \"luzatuko\" dira jatorrizko hitz bakoitzari dagozkion azpihitz guztiei BIO etiketa bera emanez.\n","\n","Hona adibide bat:\n","```\n","input in sentences (3 tokens): \n","\n","                     words: Hello San Sebastian! \n","                     tags:  O     B   I\n","\n","input as organized for batch_x and batch_adjusted_tags: \n","\n","                subwords:    [CLS] Hel #lo San Sebast #ian ! [SEP] [PAD] ... [PAD]\n","                .word_ids(): None  0   0   1   2      2    2 None  None      None\n","                tags:        -1    O   O   B   I      I    I -1    -1        -1    \n","                \n","                Note that -1 tags will be ignored when computing loss\n","```\n"],"metadata":{"id":"6NviTXeO25RL"}},{"cell_type":"code","source":["class BERTSequenceLabeler(nn.Module):\n","\n","    \n","    def __init__(self, params):\n","        super().__init__()\n","    \n","        self.model_name=params[\"model_name\"]\n","        self.tokenizer = BertTokenizerFast.from_pretrained(self.model_name, do_lower_case=params[\"doLowerCase\"], do_basic_tokenize=False)\n","        self.bert = BertModel.from_pretrained(self.model_name)\n","        self.num_labels = params[\"label_length\"]\n","\n","        self.fc = nn.Linear(params[\"embedding_size\"], self.num_labels)\n","\n","    def forward(self, batch_x): \n","    \n","        bert_output = self.bert(input_ids=batch_x[\"input_ids\"],\n","                         attention_mask=batch_x[\"attention_mask\"],\n","                         token_type_ids=batch_x[\"token_type_ids\"],\n","                         output_hidden_states=True)\n","\n","        bert_hidden_states = bert_output['hidden_states']\n","\n","        # Note that the hidden states of all layers are returned, hence the use of -1 to access the states of the top layer\n","        out = bert_hidden_states[-1]\n","        out = self.fc(out)\n","\n","        return out.squeeze()\n","\n","    def get_batches(self, all_data, batch_size=32, max_toks=256):\n","            \n","        \"\"\" Get batches for input x, y data, with data tokenized according to the BERT tokenizer \n","            (and limited to a maximum number of BERT tokens \"\"\"\n","\n","        batches_x=[]\n","        batches_y=[]\n","        \n","        for i in range(0, len(all_data), batch_size):\n","\n","            current_batch=[]\n","\n","            data=all_data[i:i+batch_size]       # returns a list of word,tag pairs\n","            def extract_words(item):            # returns a list of words\n","              return [word for word,tag in item]\n","            def extract_tags(item):             # returns a list of tags\n","              return [tag for word,tag in item]\n","            sentences = [extract_words(item) for item in data]\n","\n","            # is_split_in_words=True, Tokenizer assumes input is already tokenized into words\n","            # as the tokenizer returns subwords, the `batch_x.word_ids(i)` method returns the original word index of each subword \n","            batch_x = self.tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\", max_length=max_toks, is_split_into_words=True)\n","\n","            batch_adjusted_tags = []\n","            for i, item in enumerate(data):    \n","              word_ids = batch_x.word_ids(i)     \n","              sentence = extract_words(item)\n","              tags = extract_tags(item)\n","              def get_tag_from_id(id,tags):\n","                if id==None:\n","                  return -1\n","                else:\n","                  return tag_vocab[tags[id]]\n","              adjusted_tags = [get_tag_from_id(id,tags) for id in word_ids]\n","              batch_adjusted_tags.append(adjusted_tags)\n","\n","            mapped_ids = batch_x.word_ids()\n","\n","            batches_x.append(batch_x.to(device))\n","            batches_y.append(torch.LongTensor(batch_adjusted_tags).to(device))\n","            \n","        return batches_x, batches_y\n","  "],"metadata":{"id":"pjJPKW5Usr_R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# TRASH"],"metadata":{"id":"jOv9SH_WrywL"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"2RI7PdOZ5Xh4"},"outputs":[],"source":["\n","            '''\n","            desk = line.split(\"\\t\")\n","            index_start = int(desk[1])\n","            index_end = int(desk[2])\n","            text_segment = desk[3].split(\" \")\n","            first = True\n","            #print(cont)\n","            while cont != index_end:\n","              if cont < index_start:\n","                fragment = lines[cont:index_start]\n","                print(fragment)\n","                tokens = fragment.split(\" \")\n","                for token in tokens:\n","                  if token !=\" \":\n","                    text = token+\"\\t O \\n\"\n","                    idatzi_kasua(write_directory, text)\n","                cont += len(fragment)\n","                print(cont)\n","              elif cont == index_start:\n","                print(\"sartu\")\n","                if len(text_segment) == 1: #hitz bakarrez osatuta\n","                  text = lines[cont:index_end+1] +\"\\t B-MED \\n\"\n","                  idatzi_kasua(write_directory, text)\n","                else: \n","                  for segment in text_segment:\n","                    if first:\n","                      text = lines[cont:cont+len(segment)] +\"\\t B-MED \\n\"\n","                      idatzi_kasua(write_directory, text)\n","                      first = False\n","                      cont = len(text)+1\n","                    else:\n","                      text = lines[cont:cont+len(segment)] +\"\\t I-MED \\n\"\n","                      idatzi_kasua(write_directory, text)\n","                cont = index_end\n","              lines = lines[index_end:]\n","            \n","            sartu = False\n","            print(lines)\n","            desk = line.split(\"\\t\")\n","            index_start = int(desk[1])\n","            index_end = int(desk[2])\n","            del  lines[0:len(remove_list)]\n","            remove_list = []\n","            for l in lines:\n","              print(cont)\n","              if cont == index_end:\n","                print(\"sartu\")\n","                break\n","              if l == \" \":\n","                cont += 1\n","                remove_list.append(lines.index(l))\n","              elif cont == index_start:\n","                text = l +\"\\t B-MED \\n\"\n","                sartu = True\n","                idatzi_kasua(write_directory, text)\n","                remove_list.append(lines.index(l))\n","              elif cont < index_end and sartu:\n","                sartu = False\n","                cont += len(l)\n","                text = l +\"\\t I-MED \\n\"\n","                idatzi_kasua(write_directory, text)\n","                remove_list.append(lines.index(l))\n","              else:\n","                cont += len(l) \n","                text = l +\"\\t O \\n\"\n","                idatzi_kasua(write_directory, text)\n","                remove_list.append(lines.index(l))\n","\n","\n","\n","\n","\n","\n","\n","\n","        atera = False\n","            while not atera:\n","              if cont == index_start or cont > index_start:\n","                if len(text_segment) == 1: #entity has only one word\n","                  if cont > index_start:\n","                    cont = index_start\n","                  l = lines[cont:index_end]\n","                  cont += len(l)\n","                  text = l +\"\\t B-MED \\n\"\n","                  idatzi_kasua(write_directory, text)\n","                  atera = True\n","                elif len(text_segment) > 1 : #entity has more than one word \n","                  count = 0\n","                  if cont > index_start:\n","                      cont = index_start\n","                  while cont != index_end:\n","                    if cont == index_start:\n","                      l = lines[cont:len(text_segment[0])]\n","                      text =  l +\"\\t B-MED \\n\"\n","                      idatzi_kasua(write_directory, text)\n","                    else:\n","                      l = lines[cont:len(text_segment[count])]\n","                      text = l +\"\\t I-MED \\n\"\n","                      idatzi_kasua(write_directory, text)\n","                    count += 1\n","                    cont += len(l)\n","                  atera = True\n","\n","              else:\n","                text_other_segment = tokenize(lines[cont:index_start])\n","                count = 0\n","                for l in text_other_segment:\n","                  if l != \" \":\n","                    text = l +\"\\t O \\n\"\n","                    idatzi_kasua(write_directory, text)\n","                    count += 1\n","              \n","                  cont += len(l)\n","\n","                '''\n","              \n","            \n","           "]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":20,"status":"ok","timestamp":1670330124167,"user":{"displayName":"Paula Ontalvilla","userId":"15665637065710464846"},"user_tz":-60},"id":"45qvSDDOCuSY"},"outputs":[],"source":["def readBIO2_txt(filename, write_directory):\n","    \n","    \"\"\" Read input one by line \"\"\"\n","    with open(filename) as file:\n","        lines = [] #it  will contain the tittle and abstract\n","        cont = 0\n","        remove_list = []\n","        prev_index_start = None\n","        prev_index_end = -1\n","        sartu  = False\n","        for line in file:\n","          if  \"|\" in line:\n","            text_ta = (line.split(\"|\")[2]).split(\"\\n\")[0]\n","            lines.extend(tokenize(text_ta))\n","          else:\n","            print(lines)\n","            desk = line.split(\"\\t\")\n","            index_start = int(desk[1])\n","            index_end = int(desk[2])\n","            prev = False\n","            if prev_index_start == index_start:\n","              del  lines[0:len(remove_list)-len(text_segment)]\n","            elif  index_start < prev_index_end:\n","              prev = True\n","              del lines[0:len(remove_list)-len(text_segment)]\n","            else:\n","              del  lines[0:len(remove_list)]\n","            prev_index_start = index_start\n","            prev_index_end = index_end\n","            text_segment = tokenize(desk[3])\n","            remove_list = []\n","            for l in lines:\n","              if l in text_segment: #if it is in MentionTextSegment\n","                cont += 1\n","                if len(text_segment) == 1 and text_segment[0] == l: #entity has only one word\n","                  text = \"  \".join(text_segment) + \" \" +l +\"\\t B-MED \\n\"\n","                  idatzi_kasua(write_directory, text)\n","                elif len(text_segment) > 1 and text_segment[0] == l: #entity has more than one word and l is the fist one\n","                  sartu = True\n","                  text = \"  \".join(text_segment)+ \" \"  + l +\"\\t B-MED \\n\"\n","                  idatzi_kasua(write_directory, text)\n","                elif  len(text_segment) > 1 and text_segment[0] != l and sartu: #entity has more than one word and l is not the fist one\n","                  text = \"  \".join(text_segment)+ \" \"  + l +\"\\t I-MED \\n\"\n","                  idatzi_kasua(write_directory, text)\n","                remove_list.append(lines.index(l))\n","\n","                if cont == len(text_segment):\n","                  sartu = False\n","                  cont = 0\n","                  break\n","\n","              elif  not prev: #if it is NOT  in MentionTextSegment\n","                text =   \"  \".join(text_segment)+ \" \"  + l +\"\\t O \\n\"\n","                idatzi_kasua(write_directory, text)\n","                remove_list.append(lines.index(l))\n","\n","            "]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"authorship_tag":"ABX9TyPpIn0MVU1b9vbhV1FBB3PV"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}